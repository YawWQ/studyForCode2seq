{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "788586e46a1f41d980f31cc3c8c90943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_936e265192444644a68ca9794f48d96a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_706ec976310c461b97cabe6fec35249c",
              "IPY_MODEL_7ada2897748540c5908829548734fdd3"
            ]
          }
        },
        "936e265192444644a68ca9794f48d96a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "706ec976310c461b97cabe6fec35249c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1fef49f2b3e84dd8bbb5f57c277b76ec",
            "_dom_classes": [],
            "description": "TRAIN:  73%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2704,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1984,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_756af00b82a54c3e92f1eca7fd309087"
          }
        },
        "7ada2897748540c5908829548734fdd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1aaafc1337f147f38e462795778ca570",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1984/2704 [1:11:50&lt;24:48,  2.07s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04d1c441ac3644eeb39c55263ae011e4"
          }
        },
        "1fef49f2b3e84dd8bbb5f57c277b76ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "756af00b82a54c3e92f1eca7fd309087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1aaafc1337f147f38e462795778ca570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04d1c441ac3644eeb39c55263ae011e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPHh187P1zUx",
        "colab_type": "text"
      },
      "source": [
        "# Data and lib preparation 数据及环境准备"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KvJ44lt9t3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e47a0e8-f6ea-40d6-81aa-fc0e81f52e0b"
      },
      "source": [
        "%cd ../"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcAO_xB190-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a4b31ab7-b69f-4fb9-a275-da5027cf85a6"
      },
      "source": [
        "!wget https://s3.amazonaws.com/code2seq/datasets/java-small-preprocessed.tar.gz -P data/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-08 18:25:50--  https://s3.amazonaws.com/code2seq/datasets/java-small-preprocessed.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.98.86\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.98.86|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 479663374 (457M) [application/x-tar]\n",
            "Saving to: ‘data/java-small-preprocessed.tar.gz’\n",
            "\n",
            "java-small-preproce 100%[===================>] 457.44M  32.6MB/s    in 14s     \n",
            "\n",
            "2020-09-08 18:26:04 (32.7 MB/s) - ‘data/java-small-preprocessed.tar.gz’ saved [479663374/479663374]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQQRQ9ZG9-KX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5731ac70-d60a-47ac-8896-8fafd2c3e855"
      },
      "source": [
        "!tar -xvzf data/java-small-preprocessed.tar.gz -C data/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "java-small/java-small.dict.c2s\n",
            "java-small/java-small.test.c2s\n",
            "java-small/java-small.train.c2s\n",
            "java-small/java-small.val.c2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjofETp9_aGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8f0b240-961d-4d71-c22c-e0a40b40d053"
      },
      "source": [
        "%cd data/java-small/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data/java-small\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTjwlrqd_eZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -20000 java-small.train.c2s > java-small.train_dev.c2s"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA-2ZIs2_irv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir train train_dev val test"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Opr6h7_m6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!split -d -a 6 -l 1 --additional-suffix=.txt java-small.test.c2s test/\n",
        "!split -d -a 6 -l 1 --additional-suffix=.txt java-small.val.c2s val/\n",
        "!split -d -a 6 -l 1 --additional-suffix=.txt java-small.train.c2s train/\n",
        "!split -d -a 6 -l 1 --additional-suffix=.txt java-small.train_dev.c2s train_dev/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQHdT36aCFwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "47706bc2-f283-4fe8-8e15-be65e4246eba"
      },
      "source": [
        "!mkdir runs logs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘runs’: File exists\n",
            "mkdir: cannot create directory ‘logs’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn1PkkdaC6Xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfe0kZTtDCYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "8d3c1ed7-d020-46c2-98c5-a444eaf07a92"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 20kB/s \n",
            "\u001b[?25hCollecting tensorboard==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.6.0)\n",
            "Collecting slackweb==1.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/6e/d9a72a3a1281435f5f4a7001a0771794c73b79a49c414716f6599f8bd465/slackweb-1.0.5.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.2.0->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.0.0->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.0.0->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.0.0->-r requirements.txt (line 2)) (1.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.0.0->-r requirements.txt (line 2)) (49.6.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.0.0->-r requirements.txt (line 2)) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.0.0->-r requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.0.0->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.0.0->-r requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.6.0->-r requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.6.0->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.0.0->-r requirements.txt (line 2)) (1.7.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (1.14.48)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.0.0->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (1.17.48)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim==3.6.0->-r requirements.txt (line 3)) (2.8.1)\n",
            "Building wheels for collected packages: slackweb\n",
            "  Building wheel for slackweb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for slackweb: filename=slackweb-1.0.5-cp36-none-any.whl size=1852 sha256=65c1c6fe0b5d0576507e19b395a64d18b427a2a7c042f581d0f4c83fb27480c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/85/db/daa523e8ecc46a6cee4a04fa89fa05a053295d66063d640696\n",
            "Successfully built slackweb\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, tensorboard, slackweb\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed slackweb-1.0.5 tensorboard-2.0.0 torch-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOf94YqgEXSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "e6aa0096-5373-4551-dce0-ed5b7b84a6b4"
      },
      "source": [
        "pip install -U PyYAML"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyYAML\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 6.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 8.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=bdbee103f8fe8e62866753f8252ce00a6d4e28993cfe60f37afeb1242d359ae3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iictS2bC99y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import random\n",
        "import numpy as np\n",
        "import warnings\n",
        "import logging\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import torch\n",
        "from torch import einsum\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "from src import utils, messenger"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXlRMVmYD5tT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f38b499-c073-4c3c-f242-3f60796b4fe4"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI_481CCEqQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import yaml"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Jk8sTlEiXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b225085-a0cc-4c2c-b162-d04fbe976835"
      },
      "source": [
        "yaml.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'5.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr8Efs7Q2WxR",
        "colab_type": "text"
      },
      "source": [
        "# Code2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHSevOxSDwXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_file = '/content/configs/config_code2seq.yml'\n",
        "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
        "\n",
        "# Data source\n",
        "# 从yml文件中读取配置\n",
        "DATA_HOME = config['data']['home']\n",
        "DICT_FILE = DATA_HOME + config['data']['dict']\n",
        "TRAIN_DIR = DATA_HOME + config['data']['train']\n",
        "VALID_DIR = DATA_HOME + config['data']['valid']\n",
        "TEST_DIR  = DATA_HOME + config['data']['test']\n",
        "\n",
        "# Training parameter\n",
        "batch_size = config['training']['batch_size']\n",
        "num_epochs = config['training']['num_epochs']\n",
        "lr = config['training']['lr']\n",
        "teacher_forcing_rate = config['training']['teacher_forcing_rate']\n",
        "nesterov = config['training']['nesterov']\n",
        "weight_decay = config['training']['weight_decay']\n",
        "momentum = config['training']['momentum']\n",
        "decay_ratio = config['training']['decay_ratio']\n",
        "save_name = config['training']['save_name']\n",
        "warm_up = config['training']['warm_up']\n",
        "patience = config['training']['patience']\n",
        "\n",
        "\n",
        "\n",
        "# Model parameter\n",
        "token_size = config['model']['token_size']\n",
        "hidden_size = config['model']['hidden_size']\n",
        "num_layers = config['model']['num_layers']\n",
        "bidirectional = config['model']['bidirectional']\n",
        "rnn_dropout = config['model']['rnn_dropout']\n",
        "embeddings_dropout = config['model']['embeddings_dropout']\n",
        "num_k = config['model']['num_k']\n",
        "\n",
        "# etc\n",
        "slack_url_path = config['etc']['slack_url_path']\n",
        "info_prefix = config['etc']['info_prefix']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwAIkSEdFZrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slack_url = None\n",
        "if os.path.exists(slack_url_path):\n",
        "    slack_url = yaml.load(open(slack_url_path), Loader=yaml.FullLoader)['slack_url']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPmbGvf9FvlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c39885bf-571f-423c-e961-7e6cdcf1255c"
      },
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(1)\n",
        "random_state = 42\n",
        "\n",
        "run_id = datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
        "log_file = '../logs/' + run_id + '.log'\n",
        "exp_dir = '../runs/' + run_id\n",
        "os.mkdir(exp_dir)\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s | %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filename=log_file, level=logging.DEBUG)\n",
        "msgr = messenger.Info(info_prefix, slack_url)\n",
        "\n",
        "msgr.print_msg('run_id : {}'.format(run_id))\n",
        "msgr.print_msg('log_file : {}'.format(log_file))\n",
        "msgr.print_msg('exp_dir : {}'.format(exp_dir))\n",
        "msgr.print_msg('device : {}'.format(device))\n",
        "msgr.print_msg(str(config))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "code2seq run_id : 2020-09-08--19-00-12\n",
            "code2seq log_file : ../logs/2020-09-08--19-00-12.log\n",
            "code2seq exp_dir : ../runs/2020-09-08--19-00-12\n",
            "code2seq device : cuda\n",
            "code2seq {'data': {'home': '../data', 'dict': '/java-small/java-small.dict.c2s', 'train': '/java-small/train', 'valid': '/java-small/val', 'test': '/java-small/test'}, 'training': {'batch_size': 256, 'num_epochs': 50, 'lr': 0.001, 'teacher_forcing_rate': 0.4, 'nesterov': True, 'weight_decay': 0.01, 'momentum': 0.95, 'decay_ratio': 0.95, 'save_name': '/model.pth', 'warm_up': 1, 'patience': 2}, 'model': {'token_size': 128, 'hidden_size': 64, 'num_layers': 1, 'bidirectional': True, 'rnn_dropout': 0.5, 'embeddings_dropout': 0.3, 'num_k': 200}, 'etc': {'info_prefix': 'code2seq', 'slack_url_path': '../slack/slack_url.yml'}, 'comment': 'code2seq'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFW9yd_4Fx3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 填充字符\n",
        "PAD_TOKEN = '<PAD>' \n",
        "BOS_TOKEN = '<S>' \n",
        "EOS_TOKEN = '</S>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "PAD = 0\n",
        "BOS = 1\n",
        "EOS = 2\n",
        "UNK = 3"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-3jvJAhG4Yd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "14d5cb90-7f52-444d-b866-edead2cb9644"
      },
      "source": [
        "# load vocab dict\n",
        "# 加载词汇\n",
        "with open(DICT_FILE, 'rb') as file:\n",
        "    # 读取subtoken\n",
        "    subtoken_to_count = pickle.load(file)\n",
        "    # 读取节点数\n",
        "    node_to_count = pickle.load(file) \n",
        "    # 读取target token数\n",
        "    target_to_count = pickle.load(file)\n",
        "    # 最大值k\n",
        "    max_contexts = pickle.load(file)\n",
        "    # 训练集\n",
        "    num_training_examples = pickle.load(file)\n",
        "    msgr.print_msg('Dictionaries loaded.')\n",
        "  \n",
        "# making vocab dicts for terminal subtoken, nonterminal node and target.\n",
        "\n",
        "word2id = {\n",
        "    PAD_TOKEN: PAD,\n",
        "    BOS_TOKEN: BOS,\n",
        "    EOS_TOKEN: EOS,\n",
        "    UNK_TOKEN: UNK,\n",
        "    }\n",
        "\n",
        "# 初始化subtoken，nodes，以及target的vocabulary\n",
        "vocab_subtoken = utils.Vocab(word2id=word2id)\n",
        "vocab_nodes = utils.Vocab(word2id=word2id)\n",
        "vocab_target = utils.Vocab(word2id=word2id)\n",
        "\n",
        "vocab_subtoken.build_vocab(list(subtoken_to_count.keys()), min_count=0)\n",
        "vocab_nodes.build_vocab(list(node_to_count.keys()), min_count=0)\n",
        "vocab_target.build_vocab(list(target_to_count.keys()), min_count=0)\n",
        "\n",
        "vocab_size_subtoken = len(vocab_subtoken.id2word)\n",
        "vocab_size_nodes = len(vocab_nodes.id2word)\n",
        "vocab_size_target = len(vocab_target.id2word)\n",
        "\n",
        "\n",
        "msgr.print_msg('vocab_size_subtoken：' + str(vocab_size_subtoken))\n",
        "msgr.print_msg('vocab_size_nodes：' + str(vocab_size_nodes))\n",
        "msgr.print_msg('vocab_size_target：' + str(vocab_size_target))\n",
        "\n",
        "num_length_train = num_training_examples\n",
        "msgr.print_msg('num_examples : ' + str(num_length_train))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "code2seq Dictionaries loaded.\n",
            "code2seq vocab_size_subtoken：73908\n",
            "code2seq vocab_size_nodes：325\n",
            "code2seq vocab_size_target：11320\n",
            "code2seq num_examples : 691974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDG3W0OoHFro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader(object):\n",
        "\n",
        "    def __init__(self, data_path, batch_size, num_k, vocab_subtoken, vocab_nodes, vocab_target, shuffle=True, batch_time = False):\n",
        "        \n",
        "        \"\"\"\n",
        "        data_path : path for data \n",
        "        num_examples : total lines of data file\n",
        "        batch_size : batch size\n",
        "        num_k : max ast pathes included to one examples\n",
        "        vocab_subtoken : dict of subtoken and its id\n",
        "        vocab_nodes : dict of node simbol and its id\n",
        "        vocab_target : dict of target simbol and its id\n",
        "        \"\"\"\n",
        "        \n",
        "        self.data_path = data_path\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.num_examples = self.file_count(data_path)\n",
        "        self.num_k = num_k\n",
        "        \n",
        "        self.vocab_subtoken = vocab_subtoken\n",
        "        self.vocab_nodes = vocab_nodes\n",
        "        self.vocab_target = vocab_target\n",
        "        \n",
        "        self.index = 0\n",
        "        self.pointer = np.array(range(self.num_examples))\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        self.batch_time = batch_time\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        \n",
        "        if self.batch_time:\n",
        "            t1 = time.time()\n",
        "      \n",
        "        if self.index >= self.num_examples:\n",
        "            self.reset()\n",
        "            raise StopIteration()\n",
        "        \n",
        "        ids = self.pointer[self.index: self.index + self.batch_size]\n",
        "        seqs_S, seqs_N, seqs_E, seqs_Y = self.read_batch(ids)\n",
        "        \n",
        "        # length_k : (batch_size, k)\n",
        "        lengths_k = [len(ex) for ex in seqs_N]\n",
        "        \n",
        "        # flattening (batch_size, k, l) to (batch_size * k, l)\n",
        "        # this is useful to make torch.tensor\n",
        "        seqs_S = [symbol for k in seqs_S for symbol in k]\n",
        "        seqs_N = [symbol for k in seqs_N for symbol in k] \n",
        "        seqs_E = [symbol for k in seqs_E for symbol in k] \n",
        "        \n",
        "        # Padding\n",
        "        lengths_S = [len(s) for s in seqs_S]\n",
        "        lengths_N = [len(s) for s in seqs_N]\n",
        "        lengths_E = [len(s) for s in seqs_E]\n",
        "        lengths_Y = [len(s) for s in seqs_Y]\n",
        "        \n",
        "        max_length_S = max(lengths_S)\n",
        "        max_length_N = max(lengths_N)\n",
        "        max_length_E = max(lengths_E)\n",
        "        max_length_Y = max(lengths_Y)\n",
        "\n",
        "        padded_S = [utils.pad_seq(s, max_length_S) for s in seqs_S]\n",
        "        padded_N = [utils.pad_seq(s, max_length_N) for s in seqs_N]\n",
        "        padded_E = [utils.pad_seq(s, max_length_E) for s in seqs_E]\n",
        "        padded_Y = [utils.pad_seq(s, max_length_Y) for s in seqs_Y]\n",
        "        \n",
        "        # index for split (batch_size * k, l) into (batch_size, k, l)\n",
        "        index_N = range(len(lengths_N))\n",
        "        \n",
        "        # sort for rnn\n",
        "        seq_pairs = sorted(zip(lengths_N, index_N, padded_N, padded_S, padded_E), key=lambda p: p[0], reverse=True)\n",
        "        lengths_N, index_N, padded_N, padded_S, padded_E = zip(*seq_pairs)\n",
        "        \n",
        "        batch_S = torch.tensor(padded_S, dtype=torch.long, device=device)\n",
        "        batch_E = torch.tensor(padded_E, dtype=torch.long, device=device)\n",
        "        \n",
        "        # transpose for rnn\n",
        "        batch_N = torch.tensor(padded_N, dtype=torch.long, device=device).transpose(0, 1)\n",
        "        batch_Y = torch.tensor(padded_Y, dtype=torch.long, device=device).transpose(0, 1)\n",
        "        \n",
        "        # update index\n",
        "        self.index += self.batch_size\n",
        "                \n",
        "        if self.batch_time:\n",
        "            t2 = time.time()\n",
        "            elapsed_time = t2-t1\n",
        "            print(f\"batching time：{elapsed_time}\")\n",
        "\n",
        "        return batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N\n",
        "    \n",
        "    \n",
        "    def reset(self):\n",
        "        if self.shuffle:\n",
        "            self.pointer = shuffle(self.pointer)\n",
        "        self.index = 0 \n",
        "        \n",
        "    def file_count(self, path):\n",
        "        lst = [name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]\n",
        "        return len(lst)\n",
        "\n",
        "                \n",
        "    def read_batch(self, ids):\n",
        "        \n",
        "        seqs_S = []\n",
        "        seqs_E = []\n",
        "        seqs_N = []\n",
        "        seqs_Y = []\n",
        "        \n",
        "        for i in ids:\n",
        "            path = self.data_path + '/{:0>6d}.txt'.format(i)\n",
        "            with open(path, 'r') as f:\n",
        "                seq_S = []\n",
        "                seq_N = []\n",
        "                seq_E = []\n",
        "\n",
        "                target, *syntax_path = f.readline().split(' ')\n",
        "                target = target.split('|')\n",
        "                target = utils.sentence_to_ids(self.vocab_target, target)\n",
        "\n",
        "                # remove '' and '\\n' in sequence, java-small dataset contains many '' in a line.\n",
        "                syntax_path = [s for s in syntax_path if s != '' and s != '\\n']\n",
        "\n",
        "                # if the amount of ast path exceed the k,\n",
        "                # uniformly sample ast pathes, as described in the paper.\n",
        "                if len(syntax_path) > self.num_k:\n",
        "                    sampled_path_index = random.sample(range(len(syntax_path)) , self.num_k)\n",
        "                else :\n",
        "                    sampled_path_index = range(len(syntax_path))\n",
        "\n",
        "                for j in sampled_path_index:\n",
        "                    terminal1, ast_path, terminal2 = syntax_path[j].split(',')\n",
        "\n",
        "                    terminal1 = utils.sentence_to_ids(self.vocab_subtoken, terminal1.split('|'))\n",
        "                    ast_path = utils.sentence_to_ids(self.vocab_nodes, ast_path.split('|'))\n",
        "                    terminal2 = utils.sentence_to_ids(self.vocab_subtoken, terminal2.split('|')) \n",
        "\n",
        "                    seq_S.append(terminal1)\n",
        "                    seq_E.append(terminal2)\n",
        "                    seq_N.append(ast_path)\n",
        "\n",
        "                seqs_S.append(seq_S)\n",
        "                seqs_E.append(seq_E)\n",
        "                seqs_N.append(seq_N)\n",
        "                seqs_Y.append(target)\n",
        "\n",
        "        return seqs_S, seqs_N, seqs_E, seqs_Y"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64SI36BHIguu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size_subtoken, input_size_node, token_size, hidden_size, bidirectional = True, num_layers = 2, rnn_dropout = 0.5, embeddings_dropout = 0.25):\n",
        "        \n",
        "        \"\"\"\n",
        "        input_size_subtoken : # of unique subtoken\n",
        "        input_size_node : # of unique node symbol\n",
        "        token_size : embedded token size\n",
        "        hidden_size : size of initial state of decoder\n",
        "        rnn_dropout = 0.5 : rnn drop out ratio\n",
        "        embeddings_dropout = 0.25 : dropout ratio for context vector\n",
        "        \"\"\"\n",
        "        \n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.token_size = token_size\n",
        "\n",
        "        self.embedding_subtoken = nn.Embedding(input_size_subtoken, token_size, padding_idx=PAD)\n",
        "        self.embedding_node = nn.Embedding(input_size_node, token_size, padding_idx=PAD)\n",
        "        \n",
        "        self.lstm = nn.LSTM(token_size, token_size, num_layers = num_layers, bidirectional=bidirectional, dropout=rnn_dropout)\n",
        "        self.out = nn.Linear(token_size * 4, hidden_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(embeddings_dropout)\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, batch_S, batch_N, batch_E, lengths_k, index_N, hidden=None):\n",
        "        \n",
        "        \"\"\"\n",
        "        batch_S : (B * k, l) start terminals' subtoken of each ast path\n",
        "        batch_N : (l, B*k) nonterminals' nodes of each ast path\n",
        "        batch_E : (B * k, l) end terminals' subtoken of each ast path\n",
        "        \n",
        "        lengths_k : length of k in each example\n",
        "        index_N : index for unsorting,\n",
        "        \"\"\"\n",
        "        \n",
        "        bk_size = batch_N.shape[1]\n",
        "        output_bag = []\n",
        "        hidden_batch = []\n",
        "        \n",
        "        # (B * k, l, d)\n",
        "        encode_S = self.embedding_subtoken(batch_S)\n",
        "        encode_E = self.embedding_subtoken(batch_E)\n",
        "        \n",
        "        # encode_S (B * k, d) token_representation of each ast path\n",
        "        encode_S = encode_S.sum(1)\n",
        "        encode_E = encode_E.sum(1)\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        LSTM Outputs: output, (h_n, c_n)\n",
        "        output (seq_len, batch, num_directions * hidden_size)\n",
        "        h_n    (num_layers * num_directions, batch, hidden_size) : tensor containing the hidden state for t = seq_len.\n",
        "        c_n    (num_lay\n",
        "        \"\"\"\n",
        "        \n",
        "        # emb_N :(l, B*k, d)\n",
        "        emb_N = self.embedding_node(batch_N)\n",
        "        packed = pack_padded_sequence(emb_N, lengths_N)\n",
        "        output, (hidden, cell) = self.lstm(packed, hidden)\n",
        "        #output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        # hidden (num_layers * num_directions, batch, hidden_size)\n",
        "        # only last layer, (num_directions, batch, hidden_size)\n",
        "        hidden = hidden[-self.num_directions:, :, :]\n",
        "        \n",
        "        # -> (Bk, num_directions, hidden_size)\n",
        "        hidden = hidden.transpose(0, 1)\n",
        "        \n",
        "        # -> (Bk, 1, hidden_size * num_directions)\n",
        "        hidden = hidden.contiguous().view(bk_size, 1, -1)\n",
        "        \n",
        "        # encode_N (Bk, hidden_size * num_directions)\n",
        "        encode_N = hidden.squeeze(1)\n",
        "        \n",
        "        # encode_SNE  : (B*k, hidden_size * num_directions + 2)\n",
        "        encode_SNE = torch.cat([encode_N, encode_S, encode_E], dim=1)\n",
        "        \n",
        "        # encode_SNE  : (B*k, d)\n",
        "        encode_SNE = self.out(encode_SNE)\n",
        "\n",
        "        \n",
        "        # unsort as example\n",
        "        #index = torch.tensor(index_N, dtype=torch.long, device=device)\n",
        "        #encode_SNE = torch.index_select(encode_SNE, dim=0, index=index)\n",
        "        index = np.argsort(index_N)\n",
        "        encode_SNE = encode_SNE[[index]]\n",
        "        \n",
        "        # as is in  https://github.com/tech-srl/code2seq/blob/ec0ae309efba815a6ee8af88301479888b20daa9/model.py#L511\n",
        "        encode_SNE = self.dropout(encode_SNE)\n",
        "        \n",
        "        # output_bag  : [ B, (k, d) ]\n",
        "        output_bag = torch.split(encode_SNE, lengths_k, dim=0)\n",
        "        \n",
        "        # hidden_0  : (1, B, d)\n",
        "        # for decoder initial state\n",
        "        hidden_0 = [ob.mean(0).unsqueeze(dim=0) for ob in output_bag]\n",
        "        hidden_0 = torch.cat(hidden_0, dim=0).unsqueeze(dim=0)\n",
        "        \n",
        "        return output_bag, hidden_0"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNqUBIIaI7q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, rnn_dropout):\n",
        "        \"\"\"\n",
        "        hidden_size : decoder unit size, \n",
        "        output_size : decoder output size, \n",
        "        rnn_dropout : dropout ratio for rnn\n",
        "        \"\"\"\n",
        "        \n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, dropout=rnn_dropout)\n",
        "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, seqs, hidden, attn):\n",
        "        emb = self.embedding(seqs)\n",
        "        _, hidden = self.gru(emb, hidden)\n",
        "        \n",
        "        output = torch.cat((hidden, attn), 2)\n",
        "        output = self.out(output)\n",
        "        \n",
        "        return output, hidden"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl9lCfTvNn3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder_with_Attention(nn.Module):\n",
        "    \n",
        "    \"\"\"Conbine Encoder and Decoder\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size_subtoken, input_size_node, token_size, output_size, hidden_size, bidirectional = True, num_layers = 2, rnn_dropout = 0.5, embeddings_dropout = 0.25):\n",
        "\n",
        "        super(EncoderDecoder_with_Attention, self).__init__()\n",
        "        self.encoder = Encoder(input_size_subtoken, input_size_node, token_size, hidden_size, bidirectional = bidirectional, num_layers = num_layers, rnn_dropout = rnn_dropout, embeddings_dropout = embeddings_dropout)\n",
        "        self.decoder = Decoder(hidden_size, output_size, rnn_dropout)\n",
        "        \n",
        "        self.W_a  = torch.rand((hidden_size, hidden_size), dtype=torch.float,device=device , requires_grad=True)\n",
        "        \n",
        "        nn.init.xavier_uniform_(self.W_a)\n",
        "        \n",
        "        \n",
        "    def forward(self, batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S, max_length_N,max_length_E,max_length_Y, lengths_k, index_N, terget_max_length, batch_Y=None, use_teacher_forcing=False):\n",
        "\n",
        "        # Encoder\n",
        "        encoder_output_bag, encoder_hidden = \\\n",
        "          self.encoder(batch_S, batch_N, batch_E, lengths_k, index_N)\n",
        "        \n",
        "        _batch_size = len(encoder_output_bag)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        \n",
        "        # make initial input for decoder\n",
        "        decoder_input = torch.tensor([BOS] * _batch_size, dtype=torch.long, device=device)\n",
        "        decoder_input = decoder_input.unsqueeze(0)  # (1, batch_size)\n",
        "        \n",
        "        # output holder\n",
        "        decoder_outputs = torch.zeros(terget_max_length, _batch_size, self.decoder.output_size, device=device)\n",
        "        \n",
        "        #print('=' * 20)\n",
        "        for t in range(terget_max_length):\n",
        "            \n",
        "            # ct\n",
        "            ct = self.attention(encoder_output_bag, decoder_hidden, lengths_k)\n",
        "            \n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, ct)\n",
        "            \n",
        "            #print(decoder_output.max(-1)[1])\n",
        "            \n",
        "            decoder_outputs[t] = decoder_output\n",
        "            \n",
        "            # Teacher Forcing\n",
        "            if use_teacher_forcing and batch_Y is not None:\n",
        "                decoder_input = batch_Y[t].unsqueeze(0)\n",
        "            else: \n",
        "                decoder_input = decoder_output.max(-1)[1]\n",
        "        \n",
        "        return decoder_outputs\n",
        "    \n",
        "    def attention(self, encoder_output_bag, hidden, lengths_k):\n",
        "        \n",
        "        \"\"\"\n",
        "        encoder_output_bag : (batch, k, hidden_size) bag of embedded ast path\n",
        "        hidden : (1 , batch, hidden_size):\n",
        "        lengths_k : (batch, 1) length of k in each example\n",
        "        \"\"\"\n",
        "        \n",
        "        # e_out : (batch * k, hidden_size)\n",
        "        e_out = torch.cat(encoder_output_bag, dim=0)\n",
        "        \n",
        "        # e_out : (batch * k(i), hidden_size(j))\n",
        "        # self.W_a  : [hidden_size(j), hidden_size(k)]\n",
        "        # ha -> : [batch * k(i), hidden_size(k)]\n",
        "        ha = einsum('ij,jk->ik', e_out, self.W_a)\n",
        "        \n",
        "        # ha -> : [batch, (k, hidden_size)]\n",
        "        ha = torch.split(ha, lengths_k, dim=0)\n",
        "        \n",
        "        # dh = [batch, (1, hidden_size)]\n",
        "        hd = hidden.transpose(0,1)\n",
        "        hd = torch.unbind(hd, dim = 0)\n",
        "        \n",
        "        # _ha : (k(i), hidden_size(j))\n",
        "        # _hd : (1(k), hidden_size(j))\n",
        "        # at : [batch, ( k(i) ) ]\n",
        "        at = [F.softmax(torch.einsum('ij,kj->i', _ha, _hd), dim=0) for _ha, _hd in zip(ha, hd)]\n",
        "        \n",
        "        # a : ( k(i) )\n",
        "        # e : ( k(i), hidden_size(j))\n",
        "        # ct : [batch, (hidden_size(j)) ] -> [batch, (1, hidden_size) ]\n",
        "        ct = [torch.einsum('i,ij->j', a, e).unsqueeze(0) for a, e in zip(at, encoder_output_bag)]\n",
        "        \n",
        "        # ct [batch, hidden_size(k)]\n",
        "        # -> (1, batch, hidden_size)\n",
        "        ct = torch.cat(ct, dim=0).unsqueeze(0)\n",
        "\n",
        "        return ct\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca__owGlOIjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mce = nn.CrossEntropyLoss(size_average=False, ignore_index=PAD)\n",
        "def masked_cross_entropy(logits, target):\n",
        "    return mce(logits.view(-1, logits.size(-1)), target.view(-1))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m7_Nq_FOMaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_time = False\n",
        "train_dataloader = DataLoader(TRAIN_DIR, batch_size, num_k, vocab_subtoken, vocab_nodes, vocab_target, batch_time=batch_time, shuffle=True)\n",
        "valid_dataloader = DataLoader(VALID_DIR, batch_size, num_k, vocab_subtoken, vocab_nodes, vocab_target, shuffle=False)\n",
        "\n",
        "model_args = {\n",
        "    'input_size_subtoken' : vocab_size_subtoken,\n",
        "    'input_size_node' : vocab_size_nodes,\n",
        "    'output_size' : vocab_size_target,\n",
        "    'hidden_size' : hidden_size, \n",
        "    'token_size' : token_size,\n",
        "    'bidirectional' : bidirectional,\n",
        "    'num_layers' : num_layers,\n",
        "    'rnn_dropout' : rnn_dropout, \n",
        "    'embeddings_dropout' : embeddings_dropout\n",
        "}\n",
        "\n",
        "model = EncoderDecoder_with_Attention(**model_args).to(device)\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov = nesterov)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: decay_ratio ** epoch)\n",
        "\n",
        "fname = exp_dir + save_name\n",
        "early_stopping = utils.EarlyStopping(fname, patience, warm_up, verbose=True)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyr7hs3oOPmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N, model, optimizer=None, is_train=True):\n",
        "    model.train(is_train)\n",
        "    \n",
        "    use_teacher_forcing = is_train and (random.random() < teacher_forcing_rate)\n",
        "    \n",
        "    target_max_length = batch_Y.size(0)\n",
        "    pred_Y = model(batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N, target_max_length, batch_Y, use_teacher_forcing)\n",
        "    \n",
        "    loss = masked_cross_entropy(pred_Y.contiguous(), batch_Y.contiguous())\n",
        "    \n",
        "    if is_train:\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    batch_Y = batch_Y.transpose(0, 1).contiguous().data.cpu().tolist()\n",
        "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().T.tolist()\n",
        "    \n",
        "    \n",
        "    return loss.item(), batch_Y, pred"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDlVV8PmJrBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f543533c-9f43-4f2e-adec-fde5e9f8b6b6"
      },
      "source": [
        "TEST_DIR"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'../data/java-small/test'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qo97BdlLL8k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a430d533-db06-4052-d3e5-e0f3d12e3cda"
      },
      "source": [
        "%cd /content/configs"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/configs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLl8QUB3L1Ce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a32ee6ff-c7ff-40a8-a401-29f5292e6e4d"
      },
      "source": [
        "%cd /content/configs"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/configs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pYoS9cnL9Y9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f51c976-250a-4c3b-dc10-3999a6a5cfb5"
      },
      "source": [
        "%cd ../data/java-small/test"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data/java-small/test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5oDwLIQKEjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!split -d -a 6 -l 1 --additional-suffix=.txt /content/data/java-small/java-small.test.c2s /content/data/java-small/test/"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj5nwMTFJQ6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdf70716-9313-4f1f-a53a-b60fe33c83fb"
      },
      "source": [
        "print(train_dataloader.file_count(TEST_DIR))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7b_-np7NUFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411,
          "referenced_widgets": [
            "788586e46a1f41d980f31cc3c8c90943",
            "936e265192444644a68ca9794f48d96a",
            "706ec976310c461b97cabe6fec35249c",
            "7ada2897748540c5908829548734fdd3",
            "1fef49f2b3e84dd8bbb5f57c277b76ec",
            "756af00b82a54c3e92f1eca7fd309087",
            "1aaafc1337f147f38e462795778ca570",
            "04d1c441ac3644eeb39c55263ae011e4"
          ]
        },
        "outputId": "bcf99e3a-3c36-4f10-e486-88c2ff1568ed"
      },
      "source": [
        "#\n",
        "# Training Loop\n",
        "# \n",
        "progress_bar = False # progress bar is visible in progress_bar = False\n",
        "\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss = 0.\n",
        "    train_refs = []\n",
        "    train_hyps = []\n",
        "    valid_loss = 0.\n",
        "    valid_refs = []\n",
        "    valid_hyps = []\n",
        "    \n",
        "    # train\n",
        "    for batch in tqdm(train_dataloader, total=train_dataloader.num_examples // train_dataloader.batch_size + 1, desc='TRAIN'):\n",
        "        batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S, max_length_N,max_length_E,max_length_Y, lengths_k, index_N = batch\n",
        "        \n",
        "        loss, gold, pred = compute_loss(\n",
        "            batch_S, batch_N, batch_E, batch_Y, \n",
        "            lengths_S, lengths_N, lengths_E, lengths_Y, \n",
        "            max_length_S,max_length_N,max_length_E,max_length_Y, \n",
        "            lengths_k, index_N, model, optimizer,\n",
        "            is_train=True\n",
        "            )\n",
        "        \n",
        "        train_loss += loss\n",
        "        train_refs += gold\n",
        "        train_hyps += pred\n",
        "    \n",
        "    # valid\n",
        "    for batch in tqdm(valid_dataloader, total=valid_dataloader.num_examples // valid_dataloader.batch_size + 1, desc='VALID'):\n",
        "\n",
        "        batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N = batch\n",
        "\n",
        "        loss, gold, pred = compute_loss(\n",
        "            batch_S, batch_N, batch_E, batch_Y, \n",
        "            lengths_S, lengths_N, lengths_E, lengths_Y, \n",
        "            max_length_S,max_length_N,max_length_E,max_length_Y, \n",
        "            lengths_k, index_N, model, optimizer,\n",
        "            is_train=False\n",
        "            )\n",
        "        \n",
        "        valid_loss += loss\n",
        "        valid_refs += gold\n",
        "        valid_hyps += pred\n",
        "            \n",
        "\n",
        "    train_loss = np.sum(train_loss) / train_dataloader.num_examples\n",
        "    valid_loss = np.sum(valid_loss) / valid_dataloader.num_examples\n",
        "    \n",
        "    # F1 etc\n",
        "    train_precision, train_recall, train_f1 = utils.calculate_results_set(train_refs, train_hyps)\n",
        "    valid_precision, valid_recall, valid_f1 = utils.calculate_results_set(valid_refs, valid_hyps)\n",
        "\n",
        "    \n",
        "    early_stopping(valid_f1, model, epoch)\n",
        "    if early_stopping.early_stop:\n",
        "        msgr.print_msg(\"Early stopping\")\n",
        "        break\n",
        "    \n",
        "    msgr.print_msg('Epoch {}: train_loss: {:5.2f}  train_f1: {:2.4f}  valid_loss: {:5.2f}  valid_f1: {:2.4f}'.format(\n",
        "            epoch, train_loss, train_f1, valid_loss, valid_f1))\n",
        "    \n",
        "    print('-'*80)\n",
        "    \n",
        "    scheduler.step()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "788586e46a1f41d980f31cc3c8c90943",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='TRAIN', max=2704.0, style=ProgressStyle(description_width…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-5664bf0e19c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mmax_length_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length_N\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length_E\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlengths_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-e4f9ab8d59c5>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S, max_length_N, max_length_E, max_length_Y, lengths_k, index_N, model, optimizer, is_train)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtarget_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpred_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length_N\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length_E\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_max_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-88ec15733405>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S, max_length_N, max_length_E, max_length_Y, lengths_k, index_N, terget_max_length, batch_Y, use_teacher_forcing)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# ct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-88ec15733405>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(self, encoder_output_bag, hidden, lengths_k)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# _hd : (1(k), hidden_size(j))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# at : [batch, ( k(i) ) ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ij,kj->i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_ha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# a : ( k(i) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-88ec15733405>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# _hd : (1(k), hidden_size(j))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# at : [batch, ( k(i) ) ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ij,kj->i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_ha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# a : ( k(i) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggosA6791ZP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EncoderDecoder_with_Attention(**model_args).to(device)\n",
        "\n",
        "fname = exp_dir + save_name\n",
        "ckpt = torch.load(fname)\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBGPuIum1dx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test_dataloader = DataLoader(TEST_DIR, batch_size, num_k, vocab_subtoken, vocab_nodes, vocab_target, batch_time=batch_time, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGlH7FkW1fbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "refs_list = []\n",
        "hyp_list = []\n",
        "\n",
        "for batch in tqdm(test_dataloader,\n",
        "                      total=test_dataloader.num_examples // test_dataloader.batch_size + 1,\n",
        "                      desc='TEST'):\n",
        "    \n",
        "    batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N = batch\n",
        "    target_max_length = batch_Y.size(0)\n",
        "    use_teacher_forcing = False\n",
        "    \n",
        "    pred_Y = model(batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N, target_max_length, batch_Y, use_teacher_forcing)\n",
        "    \n",
        "    refs = batch_Y.transpose(0, 1).contiguous().data.cpu().tolist()[0]\n",
        "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().T.tolist()[0]\n",
        "    \n",
        "    refs_list.append(refs)\n",
        "    hyp_list.append(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wO2St4P1hQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msgr.print_msg('Tested model : ' + fname)\n",
        "\n",
        "test_precision, test_recall, test_f1 = utils.calculate_results(refs_list, hyp_list)\n",
        "msgr.print_msg('Test : precision {:1.5f}, recall {:1.5f}, f1 {:1.5f}'.format(test_precision, test_recall, test_f1))\n",
        "\n",
        "test_precision, test_recall, test_f1 = utils.calculate_results_set(refs_list, hyp_list)\n",
        "msgr.print_msg('Test(set) : precision {:1.5f}, recall {:1.5f}, f1 {:1.5f}'.format(test_precision, test_recall, test_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDGU1LbJ1n2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_time = False\n",
        "test_dataloader = DataLoader(TEST_DIR, 1, num_k, vocab_subtoken, vocab_nodes, vocab_target, batch_time=batch_time, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwEJYJzx1qVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N = next(test_dataloader)\n",
        "\n",
        "sentence_Y = ' '.join(utils.ids_to_sentence(vocab_target, batch_Y.data.cpu().numpy()[:-1, 0]))\n",
        "msgr.print_msg('tgt: {}'.format(sentence_Y))\n",
        "\n",
        "target_max_length = batch_Y.size(0)\n",
        "use_teacher_forcing = False\n",
        "output = model(batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N, target_max_length, batch_Y, use_teacher_forcing)\n",
        "\n",
        "output = output.max(dim=-1)[1].view(-1).data.cpu().tolist()\n",
        "output_sentence = ' '.join(utils.ids_to_sentence(vocab_target, utils.trim_eos(output)))\n",
        "msgr.print_msg('out: {}'.format(output_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}